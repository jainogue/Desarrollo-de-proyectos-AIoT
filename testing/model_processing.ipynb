{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script para procesar los datos  recojidos en el testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"model_CNN_float32\"\n",
    "\n",
    "# Cargar el CSV de ground truth\n",
    "# Se espera que tenga columnas: start_time_ms, end_time_ms, label\n",
    "annotations = pd.read_csv(\"audio_for_test/audio_test_10min.csv\")\n",
    "\n",
    "# Convertir tiempos de milisegundos a segundos\n",
    "annotations['start_sec'] = annotations['start_time_ms'] / 1000.0\n",
    "annotations['end_sec'] = annotations['end_time_ms'] / 1000.0\n",
    "\n",
    "# Cargar el csv de resultados experimentales\n",
    "results = pd.read_csv(f\"../Results/model_results/{MODEL_NAME}_results.csv\")\n",
    "\n",
    "# Convertir tiempo a milisegundos\n",
    "results['time_sec'] = results['timestamp'] / 1000.0\n",
    "\n",
    "#Función para normalizar las etiquetas a minúsculas.\n",
    "def normalize(state):\n",
    "    return state.strip().lower()\n",
    "\n",
    "# Normalizar las etiquetas (se espera que en ambos CSV se use \"Llanto\" o \"Ruido\")\n",
    "results['norm_detected'] = results['detected_state'].apply(normalize)\n",
    "annotations['norm_label'] = annotations['label'].apply(normalize)\n",
    "\n",
    "def get_ground_truth_at_time(t, results_df):\n",
    "    row = results_df[(results_df['start_sec'] <= t) & (results_df['end_sec'] >= t)]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['norm_label']\n",
    "    else:\n",
    "        return \"none\"\n",
    "\n",
    "results['ground_truth'] = results['time_sec'].apply(lambda t: get_ground_truth_at_time(t, annotations))\n",
    "\n",
    "# Determinar si la predicción es correcta\n",
    "results['correct'] = results.apply(lambda row: row['norm_detected'] == row['ground_truth'], axis=1)\n",
    "\n",
    "# Función para determinar si una ventana está reconocida\n",
    "def window_recognized(start, end, gt_label, results_df):\n",
    "    # Filtrar las predicciones cuyo tiempo (time_sec) esté en el intervalo [start, end]\n",
    "    window_results = results_df[(results_df['time_sec'] >= start) & (results_df['time_sec'] <= end)]\n",
    "    # La ventana se considera reconocida si al menos una predicción coincide con la etiqueta de ground truth\n",
    "    return any(window_results['norm_detected'] == gt_label)\n",
    "\n",
    "# Calcular la columna 'recognized' en annotations\n",
    "annotations['recognized'] = annotations.apply(\n",
    "    lambda row: window_recognized(row['start_sec'], row['end_sec'], row['norm_label'], results), axis=1\n",
    ")\n",
    "\n",
    "# Cálculo de Métricas y Matriz de Confusión\n",
    "results['TP'] = ((results['norm_detected'] == \"llanto\") & (results['ground_truth'] == \"llanto\")).astype(int)\n",
    "results['FN'] = ((results['norm_detected'] == \"ruido\") & (results['ground_truth'] == \"llanto\")).astype(int)\n",
    "results['FP'] = ((results['norm_detected'] == \"llanto\") & (results['ground_truth'] == \"ruido\")).astype(int)\n",
    "results['TN'] = ((results['norm_detected'] == \"ruido\") & (results['ground_truth'] == \"ruido\")).astype(int)\n",
    "\n",
    "TP = results['TP'].sum() #True Positives (Llanto detectado)\n",
    "FN = results['FN'].sum() #False Negatives (Llanto no detectado)\n",
    "FP = results['FP'].sum() #False Positives (Ruido no detectado)\n",
    "TN = results['TN'].sum() #True Negative (Ruido detectado)\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "avg_inference_time = np.mean(results['inference_time_ms'])\n",
    "avg_ram_used = np.mean(results['used_ram']).astype(int)\n",
    "\n",
    "print(f\"Nivel Individual:\")\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Exactitud (Accuracy): {accuracy:.2f}\")\n",
    "print(f\"Especificidad: {specificity:.2f}\")\n",
    "print(f\"Tiempo medio de inferencia: {avg_inference_time:.2f} ms\")\n",
    "print(f\"Memoria ram media utilizada {avg_ram_used} bytes\")\n",
    "\n",
    "conf_matrix = np.array([[TP, FN],\n",
    "                        [FP, TN]])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Predicted: Llanto\", \"Predicted: Ruido\"],\n",
    "            yticklabels=[\"Real: Llanto\", \"Real: Ruido\"])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.savefig(f\"../Results/model_graphs/{MODEL_NAME}_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configurar la figura\n",
    "fig, ax = plt.subplots(figsize=(15, 6.5))\n",
    "\n",
    "# Dibujar cada intervalo con un rectángulo y un símbolo encima\n",
    "for idx, row in annotations.iterrows():\n",
    "    start, end = row['start_sec'], row['end_sec']\n",
    "    gt_label = row['norm_label']\n",
    "    recognized = row['recognized']\n",
    "\n",
    "    # Asignar color de fondo según la etiqueta real\n",
    "    if gt_label == \"llanto\":\n",
    "        face_color = 'blue'\n",
    "    elif gt_label == \"ruido\":\n",
    "        face_color = 'gray'\n",
    "    else:\n",
    "        face_color = 'orange'\n",
    "\n",
    "    # Borde verde si se reconoció, rojo si no\n",
    "    symbol_color = 'green' if recognized else 'red'\n",
    "\n",
    "    # Dibujar el rectángulo\n",
    "    rect = patches.Rectangle((start, 0), end - start, 1, facecolor=face_color,\n",
    "                             alpha=0.3, edgecolor=\"black\", linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    symbol = \"✓\" if recognized else \"✗\"\n",
    "    ax.text((start + end) / 2, -0.1, symbol, ha='center', va='center',\n",
    "            fontsize=16, color=symbol_color)\n",
    "\n",
    "for _, row in results.iterrows():\n",
    "    t = row['time_sec']\n",
    "    if row['correct']:\n",
    "        ax.plot(t, 0.5, '.', color='green', markersize=6)\n",
    "    else:\n",
    "        ax.plot(t, 0.5, '.', color='red', markersize=8)\n",
    "\n",
    "# Configuración de ejes y título\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel(\"Tiempo (s)\", fontsize=12)\n",
    "ax.set_yticks([])\n",
    "ax.set_title(f\"Visualización de desempeño del modelo {MODEL_NAME}\", fontsize=14)\n",
    "\n",
    "legend_elements = [\n",
    "    patches.Patch(facecolor=\"blue\", alpha=0.3, label=\"Ground Truth: Llanto\"),\n",
    "    patches.Patch(facecolor=\"grey\", alpha=0.3, label=\"Ground Truth: Ruido\"),\n",
    "    mlines.Line2D([], [], color=\"green\", marker=\".\", linestyle=\"None\", markersize=6, label=\"Predicción Correcta\"),\n",
    "    mlines.Line2D([], [], color=\"red\", marker=\".\", linestyle=\"None\", markersize=8, label=\"Predicción Incorrecta\"),\n",
    "    mlines.Line2D([], [], color='green', marker='$✓$', linestyle='None',\n",
    "                  markersize=12, label='Ventana reconocida'),\n",
    "    mlines.Line2D([], [], color='red', marker='$✗$', linestyle='None',\n",
    "                  markersize=12, label='Ventana no reconocida')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Script para guardar los  datos  recojidos en cada testeo en el csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = f\"../Results/model_results/{MODEL_NAME}_results.csv\"\n",
    "df_res = pd.read_csv(results_file)\n",
    "\n",
    "\n",
    "metrics_row = {\n",
    "    \"model_name\":            MODEL_NAME,\n",
    "    \"precision\":             precision,\n",
    "    \"sensibilidad\":         recall,\n",
    "    \"f1\":                    f1,\n",
    "    \"exactitud\":             accuracy,\n",
    "    \"especificidad\":         specificity,\n",
    "    \"avg_inference_time_ms\": avg_inference_time,\n",
    "    \"avg_used_ram\":          avg_ram_used,\n",
    "}\n",
    "\n",
    "metrics_file = \"data/general_metrics.csv\"\n",
    "fieldnames = [\n",
    "    \"model_name\",\n",
    "    \"precision\",\n",
    "    \"sensibilidad\",\n",
    "    \"f1\",\n",
    "    \"exactitud\",\n",
    "    \"especificidad\",\n",
    "    \"avg_inference_time_ms\",\n",
    "    \"avg_used_ram\",\n",
    "]\n",
    "\n",
    "if os.path.exists(metrics_file):\n",
    "    df = pd.read_csv(metrics_file)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=fieldnames)\n",
    "\n",
    "if MODEL_NAME in df[\"model_name\"].values:\n",
    "    df.loc[df[\"model_name\"] == MODEL_NAME, fieldnames] = pd.DataFrame([metrics_row])\n",
    "else:\n",
    "    df = pd.concat([df, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "\n",
    "df.to_csv(metrics_file, index=False)\n",
    "print(f\"Métricas guardadas en {metrics_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
