{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import regularizers,  layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, Conv2D, BatchNormalization, Activation, Add,\n",
    "                                     MaxPooling2D, GlobalAveragePooling2D, Dense, Reshape, MaxPooling1D, Flatten, Dropout, LSTM, TimeDistributed, Bidirectional,SeparableConv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar datos para  entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente:\n",
      "X_train: (2014, 16000), y_train: (2014, 2)\n",
      "X_test: (504, 16000), y_test: (504, 2)\n",
      "\n",
      "Distribución de etiquetas en y_train:\n",
      "Etiqueta 0: 1007 ejemplos\n",
      "Etiqueta 1: 1007 ejemplos\n",
      "\n",
      "Distribución de etiquetas en y_test:\n",
      "Etiqueta 0: 252 ejemplos\n",
      "Etiqueta 1: 252 ejemplos\n",
      "\n",
      "Datasets creados y listos para entrenar.\n"
     ]
    }
   ],
   "source": [
    "# Rutas de los datos procesados\n",
    "TRAIN_PATH = \"../data_train/processed_audio/train.npz\"\n",
    "TEST_PATH = \"../data_train/processed_audio/test.npz\"\n",
    "\n",
    "# Cargar datos\n",
    "data_train = np.load(TRAIN_PATH)\n",
    "data_test = np.load(TEST_PATH)\n",
    "\n",
    "X_train, y_train = data_train[\"X\"], data_train[\"y\"]\n",
    "X_test, y_test = data_test[\"X\"], data_test[\"y\"]\n",
    "\n",
    "# Verificar forma de los datos\n",
    "print(\"Datos cargados correctamente:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de entrenamiento\n",
    "labels_train = np.argmax(y_train, axis=1)\n",
    "unique_labels_train, counts_train = np.unique(labels_train, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_train:\")\n",
    "for label, count in zip(unique_labels_train, counts_train):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de prueba\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "unique_labels_test, counts_test = np.unique(labels_test, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_test:\")\n",
    "for label, count in zip(unique_labels_test, counts_test):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Crear `tf.data.Dataset` para optimizar el entrenamiento\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE  # Ajuste automático del prefetching\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train))  # Mezclar datos\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"\\nDatasets creados y listos para entrenar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: NN simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                512032    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 512,594\n",
      "Trainable params: 512,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "num_features = X_train.shape[1]  # Longitud de entrada (1 segundo de audio a 16kHz)\n",
    "num_classes = y_train.shape[1]   # 2 categorías (cry, noise)\n",
    "\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential([\n",
    "    Input(shape=(num_features,)),  # Capa de entrada\n",
    "    Dense(32, activation='relu'), # Capa oculta\n",
    "    Dense(16, activation='relu'), # Capa oculta\n",
    "    Dense(num_classes, activation='softmax')  # Capa de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6359 - accuracy: 0.6216 - val_loss: 0.5582 - val_accuracy: 0.7222\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2659 - accuracy: 0.9394 - val_loss: 0.5417 - val_accuracy: 0.7639\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0888 - accuracy: 0.9886 - val_loss: 0.6717 - val_accuracy: 0.7421\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.9926 - val_loss: 0.8876 - val_accuracy: 0.6825\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0348 - accuracy: 0.9945 - val_loss: 1.0809 - val_accuracy: 0.6964\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # Detener si no hay mejora en la validación tras 3 épocas. Se  queda con los mejores pesos.\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_NN.h5\", save_best_only=True, monitor='val_loss') # Guardar el mejor modelo basado en la validación.\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: NN avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                1024064   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,027,306\n",
      "Trainable params: 1,027,066\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(num_features,)),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(8, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 3s 17ms/step - loss: 0.9165 - accuracy: 0.5268 - val_loss: 0.6687 - val_accuracy: 0.5913\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7436 - accuracy: 0.5824 - val_loss: 0.6565 - val_accuracy: 0.5992\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.6877 - accuracy: 0.6107 - val_loss: 0.6436 - val_accuracy: 0.6230\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.6273 - accuracy: 0.6514 - val_loss: 0.6165 - val_accuracy: 0.7044\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5935 - accuracy: 0.6832 - val_loss: 0.5881 - val_accuracy: 0.6984\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5510 - accuracy: 0.7274 - val_loss: 0.5794 - val_accuracy: 0.6825\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.4986 - accuracy: 0.7656 - val_loss: 0.5698 - val_accuracy: 0.6825\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.4613 - accuracy: 0.7865 - val_loss: 0.5792 - val_accuracy: 0.6647\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.4072 - accuracy: 0.8406 - val_loss: 0.5966 - val_accuracy: 0.6786\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3479 - accuracy: 0.8600 - val_loss: 0.6231 - val_accuracy: 0.6825\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_NNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: CNN simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " add_channel (Reshape)       (None, 16000, 1)          0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 3998, 16)          160       \n",
      "                                                                 \n",
      " pool1 (MaxPooling1D)        (None, 999, 16)           0         \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 248, 8)            1160      \n",
      "                                                                 \n",
      " pool2 (MaxPooling1D)        (None, 62, 8)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 496)               0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 16)                7952      \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,306\n",
      "Trainable params: 9,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Reshape((num_features, 1), input_shape=(num_features,), name=\"add_channel\"),\n",
    "\n",
    "    Conv1D(16, kernel_size=9, strides=4, activation='relu', name=\"conv1\"),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool1\"),\n",
    "\n",
    "    Conv1D(8, kernel_size=9, strides=4, activation='relu', name=\"conv2\"),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool2\"),\n",
    "\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(16, activation='relu', name=\"dense1\"),\n",
    "\n",
    "    Dense(2, activation='softmax', name=\"output\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 4s 47ms/step - loss: 0.6271 - accuracy: 0.6693 - val_loss: 0.5134 - val_accuracy: 0.7738\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 0.4931 - accuracy: 0.7915 - val_loss: 0.4377 - val_accuracy: 0.8234\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 3s 44ms/step - loss: 0.4524 - accuracy: 0.8128 - val_loss: 0.4982 - val_accuracy: 0.7857\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 3s 39ms/step - loss: 0.4204 - accuracy: 0.8292 - val_loss: 0.4002 - val_accuracy: 0.8373\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.3893 - accuracy: 0.8530 - val_loss: 0.3917 - val_accuracy: 0.8433\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 2s 40ms/step - loss: 0.3679 - accuracy: 0.8525 - val_loss: 0.3761 - val_accuracy: 0.8532\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 3s 40ms/step - loss: 0.3533 - accuracy: 0.8570 - val_loss: 0.3764 - val_accuracy: 0.8571\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 3s 46ms/step - loss: 0.3232 - accuracy: 0.8749 - val_loss: 0.3495 - val_accuracy: 0.8611\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.3134 - accuracy: 0.8714 - val_loss: 0.3535 - val_accuracy: 0.8492\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.2792 - accuracy: 0.8987 - val_loss: 0.3537 - val_accuracy: 0.8611\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.2623 - accuracy: 0.8923 - val_loss: 0.3297 - val_accuracy: 0.8829\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 3s 43ms/step - loss: 0.2566 - accuracy: 0.9022 - val_loss: 0.3713 - val_accuracy: 0.8393\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.2235 - accuracy: 0.9171 - val_loss: 0.3203 - val_accuracy: 0.8829\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.2141 - accuracy: 0.9235 - val_loss: 0.3362 - val_accuracy: 0.8651\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 2s 40ms/step - loss: 0.1823 - accuracy: 0.9310 - val_loss: 0.3232 - val_accuracy: 0.8690\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 0.1756 - accuracy: 0.9350 - val_loss: 0.3246 - val_accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_CNN1.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: CNN avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " add_channel (Reshape)       (None, 16000, 1)          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 16000, 64)         640       \n",
      "                                                                 \n",
      " pool1 (MaxPooling1D)        (None, 4000, 64)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4000, 32)          18464     \n",
      "                                                                 \n",
      " pool2 (MaxPooling1D)        (None, 1000, 32)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1000, 16)          4624      \n",
      "                                                                 \n",
      " pool3 (MaxPooling1D)        (None, 250, 16)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4000)              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 64)                256064    \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 279,922\n",
      "Trainable params: 279,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes  = y_train.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Reshape((num_features, 1), input_shape=(num_features,), name=\"add_channel\"),\n",
    "\n",
    "    Conv1D(64, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool1\"),\n",
    "\n",
    "    Conv1D(32, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool2\"),\n",
    "\n",
    "    Conv1D(16, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool3\"),\n",
    "\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(64, activation='relu', name=\"dense1\"),\n",
    "\n",
    "    Dense(num_classes, activation='softmax', name=\"output\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 35s 548ms/step - loss: 0.5529 - accuracy: 0.7284 - val_loss: 0.4410 - val_accuracy: 0.8155\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 34s 538ms/step - loss: 0.3990 - accuracy: 0.8357 - val_loss: 0.3696 - val_accuracy: 0.8591\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 34s 540ms/step - loss: 0.3327 - accuracy: 0.8674 - val_loss: 0.3368 - val_accuracy: 0.8690\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 34s 542ms/step - loss: 0.2601 - accuracy: 0.8997 - val_loss: 0.3818 - val_accuracy: 0.8433\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 33s 524ms/step - loss: 0.2178 - accuracy: 0.9156 - val_loss: 0.2930 - val_accuracy: 0.8948\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 34s 535ms/step - loss: 0.1416 - accuracy: 0.9518 - val_loss: 0.2960 - val_accuracy: 0.9087\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 33s 527ms/step - loss: 0.1168 - accuracy: 0.9588 - val_loss: 0.2969 - val_accuracy: 0.9187\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 34s 534ms/step - loss: 0.0613 - accuracy: 0.9782 - val_loss: 0.3449 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_CNNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos para MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente:\n",
      "X_train: (2012, 49, 13), y_train: (2012, 2)\n",
      "X_test: (504, 49, 13), y_test: (504, 2)\n",
      "\n",
      "Distribución de etiquetas en y_train:\n",
      "Etiqueta 0: 1005 ejemplos\n",
      "Etiqueta 1: 1007 ejemplos\n",
      "\n",
      "Distribución de etiquetas en y_test:\n",
      "Etiqueta 0: 252 ejemplos\n",
      "Etiqueta 1: 252 ejemplos\n",
      "Datasets listos para entrenar.\n"
     ]
    }
   ],
   "source": [
    "# Rutas de los datos procesados\n",
    "TRAIN_PATH = \"../data_train/processed_MFCC/train.npz\"\n",
    "TEST_PATH = \"../data_train/processed_MFCC/test.npz\"\n",
    "\n",
    "# Cargar datos procesados\n",
    "data_train = np.load(TRAIN_PATH)\n",
    "data_test = np.load(TEST_PATH)\n",
    "\n",
    "X_train, y_train = data_train[\"X\"], data_train[\"y\"]\n",
    "X_test, y_test = data_test[\"X\"], data_test[\"y\"]\n",
    "\n",
    "# Verificar forma de los datos\n",
    "print(\"Datos cargados correctamente:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de entrenamiento\n",
    "labels_train = np.argmax(y_train, axis=1)\n",
    "unique_labels_train, counts_train = np.unique(labels_train, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_train:\")\n",
    "for label, count in zip(unique_labels_train, counts_train):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de prueba\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "unique_labels_test, counts_test = np.unique(labels_test, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_test:\")\n",
    "for label, count in zip(unique_labels_test, counts_test):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.cast(X_train, tf.float32), y_train))\n",
    "    .cache()\n",
    "    .shuffle(buffer_size=len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.cast(X_test, tf.float32), y_test))\n",
    "    .cache()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Datasets listos para entrenar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 49, 13, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 49, 13, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 49, 13, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 6, 32)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 24, 6, 32)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 6, 64)         18496     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 24, 6, 64)        256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 3, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 12, 3, 64)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,850\n",
      "Trainable params: 166,658\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "        Input(shape=input_shape),\n",
    "        Reshape(input_shape + (1,)),\n",
    "\n",
    "\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 3s 35ms/step - loss: 0.6030 - accuracy: 0.7992 - val_loss: 0.5648 - val_accuracy: 0.6706\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 2s 33ms/step - loss: 0.2850 - accuracy: 0.8733 - val_loss: 0.2810 - val_accuracy: 0.8790\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 2s 33ms/step - loss: 0.2238 - accuracy: 0.9026 - val_loss: 0.2326 - val_accuracy: 0.8988\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 2s 34ms/step - loss: 0.2009 - accuracy: 0.9225 - val_loss: 0.1842 - val_accuracy: 0.9187\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 0.1628 - accuracy: 0.9384 - val_loss: 0.2075 - val_accuracy: 0.9206\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 2s 31ms/step - loss: 0.1748 - accuracy: 0.9334 - val_loss: 0.2313 - val_accuracy: 0.9087\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 2s 31ms/step - loss: 0.1405 - accuracy: 0.9493 - val_loss: 0.2049 - val_accuracy: 0.9266\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_MFCC/model_CNN.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 49, 13, 1)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 49, 13, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 49, 13, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 24, 6, 16)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 24, 6, 16)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 6, 32)         4640      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 24, 6, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 3, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 12, 3, 32)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 3, 64)         18496     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 3, 64)        256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                24640     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,530\n",
      "Trainable params: 50,306\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Reshape(input_shape + (1,)),\n",
    "\n",
    "        Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 3s 28ms/step - loss: 0.5610 - accuracy: 0.7475 - val_loss: 0.4754 - val_accuracy: 0.7778\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.3965 - accuracy: 0.8270 - val_loss: 0.3617 - val_accuracy: 0.8552\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.3422 - accuracy: 0.8514 - val_loss: 0.3321 - val_accuracy: 0.8591\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.3054 - accuracy: 0.8658 - val_loss: 0.3234 - val_accuracy: 0.8571\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.2809 - accuracy: 0.8807 - val_loss: 0.3353 - val_accuracy: 0.8552\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.2638 - accuracy: 0.8842 - val_loss: 0.3048 - val_accuracy: 0.8651\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.2388 - accuracy: 0.9021 - val_loss: 0.2809 - val_accuracy: 0.8869\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.2377 - accuracy: 0.9046 - val_loss: 0.2863 - val_accuracy: 0.8968\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.2206 - accuracy: 0.9145 - val_loss: 0.2802 - val_accuracy: 0.8968\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.1953 - accuracy: 0.9284 - val_loss: 0.3281 - val_accuracy: 0.8810\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.1806 - accuracy: 0.9309 - val_loss: 0.2387 - val_accuracy: 0.9087\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.1800 - accuracy: 0.9240 - val_loss: 0.2123 - val_accuracy: 0.9187\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.1589 - accuracy: 0.9369 - val_loss: 0.2455 - val_accuracy: 0.9048\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.1583 - accuracy: 0.9384 - val_loss: 0.2062 - val_accuracy: 0.9266\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.1566 - accuracy: 0.9344 - val_loss: 0.2165 - val_accuracy: 0.9167\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.1468 - accuracy: 0.9443 - val_loss: 0.2038 - val_accuracy: 0.9266\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.1325 - accuracy: 0.9498 - val_loss: 0.2163 - val_accuracy: 0.9266\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.1260 - accuracy: 0.9503 - val_loss: 0.1984 - val_accuracy: 0.9405\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 0.1258 - accuracy: 0.9483 - val_loss: 0.2818 - val_accuracy: 0.9107\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.1271 - accuracy: 0.9493 - val_loss: 0.2323 - val_accuracy: 0.9266\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.1325 - accuracy: 0.9513 - val_loss: 0.2233 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_MFCC/model_CNNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
