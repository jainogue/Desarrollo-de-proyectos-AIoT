{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import regularizers,  layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, Conv2D, BatchNormalization, Activation, Add,\n",
    "                                     MaxPooling2D, GlobalAveragePooling2D, Dense, Reshape, MaxPooling1D, Flatten, Dropout, LSTM, TimeDistributed, Bidirectional,SeparableConv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar datos para  entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de los datos procesados\n",
    "TRAIN_PATH = \"../data_train/processed_audio/train.npz\"\n",
    "TEST_PATH = \"../data_train/processed_audio/test.npz\"\n",
    "\n",
    "# Cargar datos\n",
    "data_train = np.load(TRAIN_PATH)\n",
    "data_test = np.load(TEST_PATH)\n",
    "\n",
    "X_train, y_train = data_train[\"X\"], data_train[\"y\"]\n",
    "X_test, y_test = data_test[\"X\"], data_test[\"y\"]\n",
    "\n",
    "# Verificar forma de los datos\n",
    "print(\"Datos cargados correctamente:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de entrenamiento\n",
    "labels_train = np.argmax(y_train, axis=1)\n",
    "unique_labels_train, counts_train = np.unique(labels_train, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_train:\")\n",
    "for label, count in zip(unique_labels_train, counts_train):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de prueba\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "unique_labels_test, counts_test = np.unique(labels_test, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_test:\")\n",
    "for label, count in zip(unique_labels_test, counts_test):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Crear `tf.data.Dataset` para optimizar el entrenamiento\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE  # Ajuste automático del prefetching\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train))  # Mezclar datos\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"\\nDatasets creados y listos para entrenar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: NN simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "num_features = X_train.shape[1]  # Longitud de entrada (1 segundo de audio a 16kHz)\n",
    "num_classes = y_train.shape[1]   # 2 categorías (cry, noise)\n",
    "\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential([\n",
    "    Input(shape=(num_features,)),  # Capa de entrada\n",
    "    Dense(32, activation='relu'), # Capa oculta\n",
    "    Dense(16, activation='relu'), # Capa oculta\n",
    "    Dense(num_classes, activation='softmax')  # Capa de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # Detener si no hay mejora en la validación tras 3 épocas. Se  queda con los mejores pesos.\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_NN.h5\", save_best_only=True, monitor='val_loss') # Guardar el mejor modelo basado en la validación.\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: NN avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(num_features,)),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(8, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_NNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: CNN simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Reshape((num_features, 1), input_shape=(num_features,), name=\"add_channel\"),\n",
    "\n",
    "    Conv1D(16, kernel_size=9, strides=4, activation='relu', name=\"conv1\"),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool1\"),\n",
    "\n",
    "    Conv1D(8, kernel_size=9, strides=4, activation='relu', name=\"conv2\"),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool2\"),\n",
    "\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(16, activation='relu', name=\"dense1\"),\n",
    "\n",
    "    Dense(2, activation='softmax', name=\"output\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_CNN1.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: CNN avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes  = y_train.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Reshape((num_features, 1), input_shape=(num_features,), name=\"add_channel\"),\n",
    "\n",
    "    Conv1D(64, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool1\"),\n",
    "\n",
    "    Conv1D(32, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool2\"),\n",
    "\n",
    "    Conv1D(16, 9, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=4, name=\"pool3\"),\n",
    "\n",
    "    Flatten(name=\"flatten\"),\n",
    "    Dense(64, activation='relu', name=\"dense1\"),\n",
    "\n",
    "    Dense(num_classes, activation='softmax', name=\"output\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_audio/model_CNNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos para MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de los datos procesados\n",
    "TRAIN_PATH = \"../data_train/processed_MFCC/train.npz\"\n",
    "TEST_PATH = \"../data_train/processed_MFCC/test.npz\"\n",
    "\n",
    "# Cargar datos procesados\n",
    "data_train = np.load(TRAIN_PATH)\n",
    "data_test = np.load(TEST_PATH)\n",
    "\n",
    "X_train, y_train = data_train[\"X\"], data_train[\"y\"]\n",
    "X_test, y_test = data_test[\"X\"], data_test[\"y\"]\n",
    "\n",
    "# Verificar forma de los datos\n",
    "print(\"Datos cargados correctamente:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de entrenamiento\n",
    "labels_train = np.argmax(y_train, axis=1)\n",
    "unique_labels_train, counts_train = np.unique(labels_train, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_train:\")\n",
    "for label, count in zip(unique_labels_train, counts_train):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "# Imprimir distribución de etiquetas en el conjunto de prueba\n",
    "labels_test = np.argmax(y_test, axis=1)\n",
    "unique_labels_test, counts_test = np.unique(labels_test, return_counts=True)\n",
    "print(\"\\nDistribución de etiquetas en y_test:\")\n",
    "for label, count in zip(unique_labels_test, counts_test):\n",
    "    print(f\"Etiqueta {label}: {count} ejemplos\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.cast(X_train, tf.float32), y_train))\n",
    "    .cache()\n",
    "    .shuffle(buffer_size=len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tf.cast(X_test, tf.float32), y_test))\n",
    "    .cache()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Datasets listos para entrenar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "        Input(shape=input_shape),\n",
    "        Reshape(input_shape + (1,)),\n",
    "\n",
    "\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_MFCC/model_CNN.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Reshape(input_shape + (1,)),\n",
    "\n",
    "        Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"../models/not_compressed_MFCC/model_CNNA.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
